# 第 8 章 提升方法

弱分类器经过线性组合（如投票）可以得到强分类器，可提高分类性能。

提升方法是从弱分类器出发，反复学习，得到一系列弱分类器（或称为基本分类器），然后组合他们构建成一个强分类器。

>Kearns 和 Valiant 提出：在概率近似正确（probably approximately correct，PAC）学习的框架中，
>* 一个概念（一个类），如果存在一个多项式的学习算法能够学习它，并且正确率很高，那么就称这个概念是强可学习的；
>* 一个概念，如果存在一个多项式的学习算法能够学习它，学习的正确率仅比随机猜测略好，那么就称这个概念是弱可学习的。
>
>后来， Schapire 证明强可学习和弱可学习是等价的。即，在 PAC （probably approximately correct）学习的框架下，一个概念是强可学习的充分必要条件是这个概念是弱可学习的。

**提升方法的两个关键问题**

1. 在每一轮如何改变训练数据的权重或概率分布
AdaBoost 的做法是，**提高前一轮若分类器错误分类样本的权值，而降低那些被正确分类样本的权值**。因此在之后轮的训练中，没有得到正确分类的样本会得到弱分类器更大的关注。于是，分类问题被一系列弱分类器“分而治之”。

2. 如何将弱分类器组合成一个强分类器
AdaBoost 采取**加权多数表决**的方法。即，加大分类误差率小的弱分类器的权值，使其在表决中其起到更大作用，减小分类误差率大的分类器的权值，使其在表决中起较小的作用。

提升的方法有很多，大多数的提升方法都是改变训练数据的分布，最经典的就是 AdaBoost 算法。

# 8.1.1 提升方法 AdaBoost 算法
给定二分类训练数据集

$$
T = \{(x_1,y_1),(x_2,y_2),...,(x_N,y_N)\}
$$

每个样本，比方第 $i$ 个样本由特征 $x_i \in \chi \subseteq R^n$ 和其真实类别标记 $y_i \in \gamma = \{-1,+1\}$ 组成，$\chi$ 是实例空间， $\gamma$ 是类别的标记集合。

*****
**AdaBoost 算法描述**

* 输入：训练数据集 $T = \{(x_1,y_1),(x_2,y_2),...,(x_N,y_N)\}$ ，其中 $x_i \in \chi \subseteq R^n$ ， $y_i \in \gamma = \{-1,+1\}$ ；弱学习算法。
* 输出：最终分类器 $G(x).$

1. 初始化训练数据的权值分布
$$
D_1 = (w_{1,1},...,w_{1,i},...w_{1,N}),w_{1,i}=\frac{1}{N},i = 1,2,...,N
$$
*说明：假设训练数据集具有均匀的均值分布，即每个训练样本在基本分类器的学习中作用相同，这一假设保证第 1 步能够在原始数据上学习基本分类器 $G_1(x)$ 。 $w$ 的下脚标的第一个表示第几轮学习（也即当前训练到第几个分类器），第二个脚标表示训练样本在该轮学习中的下标。*
2. 对 $m = 1,2,...,N$ 顺次地执行下列操作，让 AdaBoost 反复学习基本分类器：
  1. 使用当前分布为 $D_m$ 的加权训练数据集学习，学习得到基本分类器 $G_m(x)$ ：
  $$G_m(x):\chi \rightarrow \{-1,+1\}$$
  2. 计算基本分类器 $G_m(x)$ 在加权训练数据集上的分类误差率：
  $$e_m=P(G_m(x_i) \neq y_i ) = \sum_{i=1}^{N} w_{m,i}I(G_m(x_i) \neq y_i)$$
  *说明：$w_{mi}$ 表示第 $m$ 轮（即第 $m$ 个分类器）中第 $i$ 个实例的权值， $\sum_{i=1}^{N} w_{m,i} = 1.$ 这表明， $G_m(x)$ 在加权的训练数据集上的分类误差率是被 $G_m(x)$ 误分类样本的权值之和，由此可以看出数据权值分布 $D_m$ 与基本分类器 $G_m(x)$ 的分类误差率的关系。*
  3. 计算 $G_m(x)$ 的系数
  $$\alpha_m = \frac{1}{2} \ln{\frac{1-e_m}{e_m}}$$
  这里的对数是自然对数.
  *说明：计算基本分类器 $G_m(x)$ 的系数 $\alpha_m$ ， $\alpha_m$ 表示 $G_m(x)$ 在最终分类器中的重要性。由计算该系数的公式 $\alpha_m = \frac{1}{2} \log\frac{1-e_m}{e_m}$ 的公式可知，当 $e_m \leq \frac{1}{2}$ 时，并且 $\alpha_m \geq 0$ ，并且 $\alpha_m$ 随着 $e_m$ 的减小而增大，所以分类误差率越小的基本分类器在最终分类器中的作用越大。*
  4. 更新数据集的权值分布
  $$
  D_{m+1} = (w_{m+1,1}, ..., w_{m+1,i}, w_{m+1,N})\\
  w_{m+1,i} = \frac{w_{mi}}{Z_m} \exp(-\alpha_m y_i G_m(x_i)),i=1,2,...,N
  $$
  这里， $Z_m$ 是规范化因子
  $$
  Z_m = \sum_{i=1}^{N} w_{m,i} \exp(-\alpha_m y_i G_m(x_i))
  $$
  它使 $D_{m+1}$ 成为一个概率分布。
  *说明：更新训练数据的权值分布为下一轮作准备，更新 $w_{m+1}$ 的公式也可写为：*
  $$
  w_{m+1,i}=
  \left
  \{\begin{array}{ll}
  \frac{w_{m,i}}{Z_m} e^{-\alpha_m},
  &
  G_m(x_i)=y_i\\
  \frac{w_{m,i}}{Z_m} e^{\alpha_m},
  &
  G_m(x_i)\neq y_i
  \end{array}
  \right.
  $$
  **由此可知，被基本分类器误分类样本的权值得以扩大，而被正确分类样本的权值却得以缩小。两相比较，误分类样本的权值被放大 $e^{2\alpha_m}$ 倍。因此，误分类样本在下一轮学习中会起更大的作用（通过权值体现），使得训练数据在基本分类器的学习中起不同的作用，这是 AdaBoost 的特点。**
3. 构建基本分类器的线性组合
$$
f(x) = \sum_{m=1}^{M} \alpha_m G_m(x)
$$
得到最终分类器
$$
G(x) = sign(f(x)) = sign \left [ \sum_{m=1}^{M} \alpha_m G_m(x) \right ]
$$
*说明：线性组合 $f(x)$ 通过 $M$ 轮迭代实现 $M$ 个基本分类器的加权表决。系数 $\alpha_m$ 表示了基本分类器 $G_m(x)$ 的重要性，这里，所有 $\alpha_m$ 之和并不为 $1$ 。*
** $f(x)$ 的符号决定实例 $x$ 的类， $f(x)$ 的绝对值表示分类的确信度。利用基本分类器的线性组合构建最终分类器是 AdaBoost 的另一特点。**


*****

## 8.2 AdaBoost 算法的训练误差分析
AdaBoost 最基本的特点是可在训练过程中减少误差，因为误差体现在预测与实际的类别的（乘积符号）差距， $e_m=P(G_m(x_i) \neq y_i ) = \sum_{i=1}^{N} w_{mi}I(G_m(x_i) \neq y_i)$ ，进一步说就是体现在训练数据集上的每个样本的权重上。关于这个问题有如下定理：

**定理 8.1（AdaBoost的训练误差界）** AdaBoost算法最终分类器的训练误差界为
$$
\frac{1}{N} \sum_{i=1}^{N} I(G(x_i)\neq y_i)
\leq
\frac{1}{N} \sum_{i} \exp(-y_i f(x_i))
=
\prod_{m} Z_m
$$
这里，$G(x),f(x),Z_m$ 已在前文给出。
>这里再补充一下，下面分别为计算规范化因子 $Z_m$ 、 基本分类器的线性组合 $f(x)$ 、 最终分类器 $G(x)$ 的公式：
$$
Z_m = \sum_{i=1}^{N} w_{m,i} \exp(-\alpha_m y_i G_m(x_i))\\
f(x) = \sum_{m=1}^{M} \alpha_m G_m(x)\\
G(x) = sign(f(x)) = sign \left [ \sum_{m=1}^{M} \alpha_m G_m(x) \right ]
$$

**证明** 当 $G(x_i) \neq y_i$ 时， $y_i f(x_i) < 0$ ，因而 $\exp(-y_i f(x_i)) \geq 1.$ 由此直接导出前半部分.

后半部分的推倒要用到 $Z_m$ 的定义式及 $w_{m,i}$ 的定义式的变形。

现推导如下：
$$
\frac{1}{N} \sum_i \exp(-y_i f(x_i)) \\
= 
\frac{1}{N} \sum_i \exp()
$$


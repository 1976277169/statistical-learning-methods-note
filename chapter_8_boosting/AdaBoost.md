# 第 8 章 提升方法

弱分类器经过线性组合（如投票）可以得到强分类器，可提高分类性能。

提升方法是从弱分类器出发，反复学习，得到一系列弱分类器（或称为基本分类器），然后组合他们构建成一个强分类器。

>Kearns 和 Valiant 提出：在概率近似正确（probably approximately correct，PAC）学习的框架中，
>* 一个概念（一个类），如果存在一个多项式的学习算法能够学习它，并且正确率很高，那么就称这个概念是强可学习的；
>* 一个概念，如果存在一个多项式的学习算法能够学习它，学习的正确率仅比随机猜测略好，那么就称这个概念是弱可学习的。
>
>后来， Schapire 证明强可学习和弱可学习是等价的。即，在 PAC （probably approximately correct）学习的框架下，一个概念是强可学习的充分必要条件是这个概念是弱可学习的。

**提升方法的两个关键问题**

1. 在每一轮如何改变训练数据的权重或概率分布
AdaBoost 的做法是，**提高前一轮若分类器错误分类样本的权值，而降低那些被正确分类样本的权值**。因此在之后轮的训练中，没有得到正确分类的样本会得到弱分类器更大的关注。于是，分类问题被一系列弱分类器“分而治之”。

2. 如何将弱分类器组合成一个强分类器
AdaBoost 采取**加权多数表决**的方法。即，加大分类误差率小的弱分类器的权值，使其在表决中其起到更大作用，减小分类误差率大的分类器的权值，使其在表决中起较小的作用。

提升的方法有很多，大多数的提升方法都是改变训练数据的分布，最经典的就是 AdaBoost 算法。

# 8.1.1 提升方法 AdaBoost 算法
给定二分类训练数据集

$$
T = \{(x_1,y_1),(x_2,y_2),...,(x_N,y_N)\}
$$

每个样本，比方第 $i$ 个样本由特征 $x_i \in \chi \subseteq R^n$ 和其真实类别标记 $y_i \in \gamma = \{-1,+1\}$ 组成，$\chi$ 是实例空间， $\gamma$ 是类别的标记集合。

*****
**AdaBoost 算法描述**

* 输入：训练数据集 $T = \{(x_1,y_1),(x_2,y_2),...,(x_N,y_N)\}$ ，其中 $x_i \in \chi \subseteq R^n$ ， $y_i \in \gamma = \{-1,+1\}$ ；弱学习算法。
* 输出：最终分类器 $G(x).$

1. 初始化训练数据的权值分布
$$
D_1 = (w_{1,1},...,w_{1,i},...w_{1,N}),w_{1,i}=\frac{1}{N},i = 1,2,...,N
$$
*说明：假设训练数据集具有均匀的均值分布，即每个训练样本在基本分类器的学习中作用相同，这一假设保证第 1 步能够在原始数据上学习基本分类器 $G_1(x)$ 。 $w$ 的下脚标的第一个表示第几轮学习（也即当前训练到第几个分类器），第二个脚标表示训练样本在该轮学习中的下标。*
2. 对 $m = 1,2,...,N$ 顺次地执行下列操作，让 AdaBoost 反复学习基本分类器：
  1. 使用当前分布为 $D_m$ 的加权训练数据集学习，学习得到基本分类器 $G_m(x)$ ：
  $$G_m(x):\chi \rightarrow \{-1,+1\}$$
  2. 计算基本分类器 $G_m(x)$ 在加权训练数据集上的分类误差率：
  $$e_m=P(G_m(x_i) \neq y_i ) = \sum_{i=1}^{N} w_{m,i}I(G_m(x_i) \neq y_i)$$
  *说明：$w_{mi}$ 表示第 $m$ 轮（即第 $m$ 个分类器）中第 $i$ 个实例的权值， $\sum_{i=1}^{N} w_{m,i} = 1.$ 这表明， $G_m(x)$ 在加权的训练数据集上的分类误差率是被 $G_m(x)$ 误分类样本的权值之和，由此可以看出数据权值分布 $D_m$ 与基本分类器 $G_m(x)$ 的分类误差率的关系。*
  3. 计算 $G_m(x)$ 的系数
  $$\alpha_m = \frac{1}{2} \ln{\frac{1-e_m}{e_m}}$$
  这里的对数是自然对数.
  *说明：计算基本分类器 $G_m(x)$ 的系数 $\alpha_m$ ， $\alpha_m$ 表示 $G_m(x)$ 在最终分类器中的重要性。由计算该系数的公式 $\alpha_m = \frac{1}{2} \log\frac{1-e_m}{e_m}$ 的公式可知，当 $e_m \leq \frac{1}{2}$ 时，并且 $\alpha_m \geq 0$ ，并且 $\alpha_m$ 随着 $e_m$ 的减小而增大，所以分类误差率越小的基本分类器在最终分类器中的作用越大。*
  4. 更新数据集的权值分布
  $$
  D_{m+1} = (w_{m+1,1}, ..., w_{m+1,i}, w_{m+1,N})\\
  w_{m+1,i} = \frac{w_{mi}}{Z_m} \exp(-\alpha_m y_i G_m(x_i)),i=1,2,...,N
  $$
  这里， $Z_m$ 是规范化因子
  $$
  Z_m = \sum_{i=1}^{N} w_{m,i} \exp(-\alpha_m y_i G_m(x_i))
  $$
  它使 $D_{m+1}$ 成为一个概率分布。
  *说明：更新训练数据的权值分布为下一轮作准备，更新 $w_{m+1}$ 的公式也可写为：*
  $$
  w_{m+1,i}=
  \left
  \{\begin{array}{ll}
  \frac{w_{m,i}}{Z_m} e^{-\alpha_m},
  &
  G_m(x_i)=y_i\\
  \frac{w_{m,i}}{Z_m} e^{\alpha_m},
  &
  G_m(x_i)\neq y_i
  \end{array}
  \right.
  $$
  **由此可知，被基本分类器误分类样本的权值得以扩大，而被正确分类样本的权值却得以缩小。两相比较，误分类样本的权值被放大 $e^{2\alpha_m}$ 倍。因此，误分类样本在下一轮学习中会起更大的作用（通过权值体现），使得训练数据在基本分类器的学习中起不同的作用，这是 AdaBoost 的特点。**
3. 构建基本分类器的线性组合
$$
f(x) = \sum_{m=1}^{M} \alpha_m G_m(x)
$$
得到最终分类器
$$
G(x) = sign(f(x)) = sign \left [ \sum_{m=1}^{M} \alpha_m G_m(x) \right ]
$$
*说明：线性组合 $f(x)$ 通过 $M$ 轮迭代实现 $M$ 个基本分类器的加权表决。系数 $\alpha_m$ 表示了基本分类器 $G_m(x)$ 的重要性，这里，所有 $\alpha_m$ 之和并不为 $1$ 。*
** $f(x)$ 的符号决定实例 $x$ 的类， $f(x)$ 的绝对值表示分类的确信度。利用基本分类器的线性组合构建最终分类器是 AdaBoost 的另一特点。**


*****

## 8.2 AdaBoost 算法的训练误差分析
AdaBoost 最基本的特点是可在训练过程中减少误差，因为误差体现在预测与实际的类别的（乘积符号）差距， $e_m=P(G_m(x_i) \neq y_i ) = \sum_{i=1}^{N} w_{mi}I(G_m(x_i) \neq y_i)$ ，进一步说就是体现在训练数据集上的每个样本的权重上。关于这个问题有如下定理：

**定理 8.1（AdaBoost的训练误差界）** AdaBoost算法最终分类器的训练误差界为
$$
\frac{1}{N} \sum_{i=1}^{N} I(G(x_i)\neq y_i)
\leq
\frac{1}{N} \sum_{i} \exp(-y_i f(x_i))
=
\prod_{m} Z_m
$$
这里，$G(x),f(x),Z_m$ 已在前文给出。
>这里再补充一下，下面分别为计算规范化因子 $Z_m$ 、 基本分类器的线性组合 $f(x)$ 、 最终分类器 $G(x)$ 的公式：
$$
Z_m = \sum_{i=1}^{N} w_{m,i} \exp(-\alpha_m y_i G_m(x_i))\\
f(x) = \sum_{m=1}^{M} \alpha_m G_m(x)\\
G(x) = sign(f(x)) = sign \left [ \sum_{m=1}^{M} \alpha_m G_m(x) \right ]
$$

**证明** 当 $G(x_i) \neq y_i$ 时， $y_i f(x_i) < 0$ ，因而 $\exp(-y_i f(x_i)) \geq 1.$ 由此直接导出前半部分.

后半部分的推倒要用到 $Z_m$ 及 $w_{m,i}$ 的定义式的变形。
> **$Z_m$ 及 $w_{m,i}$ 的定义式**
$$
Z_m = \sum_{i=1}^N w_{m,i} \exp(-\alpha y_i G_m(x_i)) \\
w_{m+1,i} = \frac{w_{m,i}}{Z_m} \exp(-\alpha_m y_i G_m(x_i)), i=1,2,...,N
$$

现推导如下：

$$
\begin{align}
& \frac{1}{N} \sum_i \exp(-y_i f(x_i)) \\
= & \frac{1}{N} \sum_i \exp\left [ -\sum_{m=1}^{M} \alpha_m y_i G_m(x_i) \right ],
其中 f(x_i) = \alpha_m G_m(x_i)
\\
= & \sum_i w_{1,i} \prod_{m=1}^{M} \exp(-\alpha_m y_i G_m(x_i)),
其中 \frac{1}{N} = w_{1,i} ，自然数的指数变为连乘的形式
\\
= & Z_1 \sum_i w_{2,i} \prod_{m=2}^{M} \exp(-\alpha_m y_i G_m(x_i)),这里\prod和\sum的顺序感觉有点问题
\\
= & Z_1  Z_2 \sum_i w_{3,i} \prod_{m=3}^{M} \exp(-\alpha_m y_i G_m(x_i)) \\
= & ... \\
= & Z_1 Z_2 ... Z_{M-1} \sum_i w_{M,i} \exp(-\alpha_M y_i G_M(x_i)) \\
= & \prod_{m=1}^{M} Z_m
\end{align}
$$

该定理说明，在每一轮选取适当的 $G_m$ 过程中，使得 $Z_m$ 最小，从而使训练误差下降最快。对于二分类问题，有如下定理：

**定理 8.2 （二分类问题 AdaBoost 的训练误差界）**
$$
\prod_{m=1}^{M} Z_m
=
\prod_{m=1}^{M}[ 2 \sqrt{e_m(1-e_m)}]
=
\prod_{m=1}^{M} \sqrt{(1-4\gamma_m^2)}
\le
\exp\left ( -2\sum_{m=1}^{M} \gamma_m^2 \right )
$$

这里，$\gamma = \frac{1}{2} - e_m.$

**证明** 由 $Z_m$ 及 $e_m$ 的定义式得

$$
\begin{align}
Z_m = & \sum_{i=1}^{N} w_{m,i} \exp(-\alpha_m y_i G_m(x_i)) \\
= & \sum_{y_i=G_m(x_i)} w_{m,i} \exp(-\alpha_m) + \sum_{y_i \ne G_m(x_i)} w_{m,i} \exp(\alpha_m) \\
= & (1-e_m)\exp(-\alpha_m) + e_m \exp(\alpha_m) \\
= & 2 \sqrt{e_m(1-e_m)}, 莫非是a+b\leq2\sqrt{ab},但这里应该用\leq符号按理说\\
= & \sqrt{1-4\gamma_m^2}
\end{align} \\
$$

至于不等式

$$
\prod_{m=1}^M \sqrt{(1-4\gamma_m^2)}
\le
\exp(-2\sum_{m=1}^M \gamma_m^2)
$$

则可先由 $\exp(x)$ 和 $\sqrt{1-x}$ 在点 $0$ 的泰勒展开式推出不等式 $\sqrt{(1-4\gamma_m^2)} \leq \exp(-2\gamma_m^2)$ ，进而得到。

>**泰勒公式**
>在数学中，泰勒公式是一个用函数在某点的信息描述其附近取值的公式。如果函数足够平滑的话，在已知函数在某一点的各阶导数值的情况之下，泰勒公式可以用这些导数值做系数构建一个多项式来近似函数在这一点的邻域中的值。泰勒公式还给出了这个多项式和实际的函数值之间的偏差。
>
>**泰勒公式形式**
>泰勒公式可以用（无限或者有限）若干项连加式（-级数）来表示一个函数，这些相加的项由函数在某一点（或者加上在临近的一个点的 $n+1$ 次导数）的导数求得。
>
>对于正整数 $n$ ，若函数 $f(x)$ 在闭区间 $[a,b]$ 上 $n$ 阶连续可导，且在 $(a,b)$ 上 $n+1$ 阶可导。任取 $x \in [a,b]$ 时的一个定点，则对任意 $x \in [a,b]$ 成立下式：
>$$
f(x)=\frac{f(a)}{0!} + 
\frac{f'(a)}{1!} (x-a) + 
\frac{f''(a)}{2!} (x-a)^2 + ... +
\frac{f^{(n)}(a)}{n!} (x-a)^n + 
R_n(x)
$$
>其中， $f^{(n)}(x)$ 表示 $f(x)$ 的 $n$ 阶导数，多项式称为函数 $f(x)$ 在 $a$ 处的泰勒展开式，剩余的 $R_n(x)$ 是泰勒公式的余项（这里省略），是 $(x-a)^n$ 的高阶无穷小。

**推论 8.1** 如果存在 $\gamma \gt 0$ ，对所有 $m$ ，有 $\gamma_m \geq \gamma$ ，则

$$
\frac{1}{N} \sum_{i=1}^{N} I(G(x_i) \neq y_i) \leq \exp(-2M\gamma^2)
$$

这表明在此条件下的 AdaBoost 的训练误差是以指数速率下降的。这一性质当然是很有吸引力的。

注意， AdaBoost 算法不需要知道下界 $\gamma$ 。这正是 Freund 和 Schapire 设计 AdaBoost 时所考虑的。与一些早期的提升方法不同， AdaBoost 具有适应性，即它能适应弱分类器各自的训练误差率。这也是它的名称（适应地提升，即 Adaptive Boost ）的由来。
# 第 8 章 提升方法

弱分类器经过线性组合（如投票）可以得到强分类器，可提高分类性能。

提升方法是从弱分类器出发，反复学习，得到一系列弱分类器（或称为基本分类器），然后组合他们构建成一个强分类器。

>Kearns 和 Valiant 提出：在概率近似正确（probably approximately correct，PAC）学习的框架中，
>* 一个概念（一个类），如果存在一个多项式的学习算法能够学习它，并且正确率很高，那么就称这个概念是强可学习的；
>* 一个概念，如果存在一个多项式的学习算法能够学习它，学习的正确率仅比随机猜测略好，那么就称这个概念是弱可学习的。
>
>后来， Schapire 证明强可学习和弱可学习是等价的。即，在 PAC （probably approximately correct）学习的框架下，一个概念是强可学习的充分必要条件是这个概念是弱可学习的。

**提升方法的两个关键问题**

1. 在每一轮如何改变训练数据的权重或概率分布
AdaBoost 的做法是，**提高前一轮若分类器错误分类样本的权值，而降低那些被正确分类样本的权值**。因此在之后轮的训练中，没有得到正确分类的样本会得到弱分类器更大的关注。于是，分类问题被一系列弱分类器“分而治之”。

2. 如何将弱分类器组合成一个强分类器
AdaBoost 采取**加权多数表决**的方法。即，加大分类误差率小的弱分类器的权值，使其在表决中其起到更大作用，减小分类误差率大的分类器的权值，使其在表决中起较小的作用。

提升的方法有很多，大多数的提升方法都是改变训练数据的分布，最经典的就是 AdaBoost 算法。

# 8.1 提升方法 AdaBoost 算法
给定二分类训练数据集

$$
T = \{(x_1,y_1),(x_2,y_2),...,(x_N,y_N)\}
$$

每个样本，比方第 $i$ 个样本由特征 $x_i \in \chi \subseteq R^n$ 和其真实类别标记 $y_i \in \gamma = \{-1,+1\}$ 组成，$\chi$ 是实例空间， $\gamma$ 是类别的标记集合。

*****
**AdaBoost 算法描述**

* 输入：训练数据集 $T = \{(x_1,y_1),(x_2,y_2),...,(x_N,y_N)\}$ ，其中 $x_i \in \chi \subseteq R^n$ ， $y_i \in \gamma = \{-1,+1\}$ ；弱学习算法。
* 输出：最终分类器 $G(x).$

1. 初始化训练数据的权值分布
$$
D_1 = (w_{11},...,w_{1i},...w_{1N}),w_{1i}=\frac{1}{N},i = 1,2,...,N
$$
2. 对 $m = 1,2,...,N$
  1. 使用具有权值分布 $D_m$ 的训练数据集学习，得到基本分类器
  $$G_m(x):\chi \rightarrow \{-1,+1\}$$
  2. 计算 $G_m(x)$ 在训练数据集上的分类误差率
  $$e_m=P(G_m(x_i) \neq y_i ) = \sum_{i=1}^{N} w_{mi}I(G_m(x_i) \neq y_i)$$
  3. 计算 $G_m(x)$ 的系数
  $$\alpha_m = \frac{1}{2} \ln{\frac{1-e_m}{e_m}}$$
  这里的对数是自然对数.
  4. 更新数据集的权值分布
  $$
  D_{m+1} = (w_{m+1,1}, ..., w_{m+1,i}, w_{m+1,N})\\
  w_{m+1,i} = \frac{w_{mi}}{Z_m} \exp(-\alpha_m y_i G_m(x_i)),i=1,2,...,N
  $$
  这里， $Z_m$ 是规范化因子
  $$
  Z_m = \sum_{i=1}^{N} w_{mi} \exp(-\alpha_m y_i G_m(x_i))
  $$
  它使 $D_{m+1}$ 成为一个概率分布.
3. 构建基本分类器的线性组合
$$
f(x) = \sum_{m=1}^{M} \alpha_m G_m(x)
$$
得到最终分类器
$$
G(x) = sign(f(x)) = sign\left [ \sum_{m=1}^{M} \alpha_m G_m(x) \right ]
$$
*****




